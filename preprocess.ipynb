{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e478ed-24b8-47f8-a866-6369026af3b2",
   "metadata": {},
   "source": [
    "# Spam Detection Preprocessing\n",
    "\n",
    "This notebook demonstrates the preprocessing steps for detecting spam emails using a neural network. The steps include loading data, cleaning text, tokenizing, lemmatizing, and converting text to sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b452b33a-6789-4a40-94ef-6ae9a08613e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gonzalopereyrametnik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gonzalopereyrametnik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1a0c5-a884-4313-883d-2341aba030e4",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "We load the dataset from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fa118-0a25-4c86-ba91-a43314a15b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c766db-deea-421c-bb9a-b4ee0f7329e6",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "\n",
    "This function removes non-alphabetic characters and converts text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d381d-b7b2-4f23-8d69-ad203076d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform basic text cleaning:\n",
    "    - Remove non-alphabetic characters\n",
    "    - Convert to lowercase\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  \n",
    "    text = text.lower() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb607e-950b-4fb7-9bd0-ffe3549fd3f1",
   "metadata": {},
   "source": [
    "### Tokenization and Stop Word Removal\n",
    "\n",
    "This function tokenizes text into words and removes common stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eab1b8-25ab-468e-bd1c-e072835e7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and remove stop words.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Tokenize\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f42809-b213-458f-a3aa-343363ff6d40",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "This function converts words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758d562-7405-4797-8e16-b43be594df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    \"\"\"\n",
    "    Lemmatize words to their base form.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4723615-5556-4966-979a-6d3bb61f5dc7",
   "metadata": {},
   "source": [
    "### Full Preprocessing Pipeline\n",
    "\n",
    "This function combines text cleaning, tokenization, stop word removal, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec4813-5cc3-40c1-8c52-fd7386269cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for a single text:\n",
    "    - Clean text\n",
    "    - Tokenize and remove stop words\n",
    "    - Lemmatize words\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    lemmatized_words = lemmatize_words(words)\n",
    "    return ' '.join(lemmatized_words)  # Join words back into a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcfc07-6545-4fd6-8119-aa243fffcaaf",
   "metadata": {},
   "source": [
    "### Preprocessing DataFrame\n",
    "\n",
    "This function applies the preprocessing pipeline to the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c661e7-167d-4476-be80-5466026ba2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, text_column):\n",
    "    \"\"\"\n",
    "    Apply preprocessing to a dataframe containing text data.\n",
    "    \"\"\"\n",
    "    df['cleaned_text'] = df[text_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011f035-2a44-4a88-a830-4c4038511502",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "\n",
    "This function splits the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7222f-959c-4671-a1c1-8e8097cc6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(df, text_column, label_column):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[text_column], df[label_column], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb735e9-5a5a-4f95-9a77-f227a08b3687",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "Load, preprocess, and split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b64b34-c67d-40b7-a3e1-7728451e7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/spam_ham_dataset.csv'\n",
    "text_column = 'text'\n",
    "label_column = 'label_num'\n",
    "\n",
    "# Load data\n",
    "df = load_data(file_path)\n",
    "\n",
    "# Preprocess data\n",
    "df = preprocess_dataframe(df, text_column)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = split_data(df, 'cleaned_text', label_column)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=5000)  # Adjust the number of words as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d5ffc-2f13-476b-95ef-6d8681acf0f9",
   "metadata": {},
   "source": [
    "### Tokenizing and Padding Sequences\n",
    "Convert text data to sequences of integers and pad them to ensure uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018df751-b182-4676-b7ee-140912432b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 2011  \n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6059c-a378-42dc-8153-39bbccca68af",
   "metadata": {},
   "source": [
    "### Saving Processed Data\n",
    "Save the preprocessed and tokenized data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2c8d6c-c7cb-44f5-a0cf-ed959c802828",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/X_train_padded.npy', X_train_padded)\n",
    "np.save('data/X_test_padded.npy', X_test_padded)\n",
    "np.save('data/y_train.npy', y_train)\n",
    "np.save('data/y_test.npy', y_test)\n",
    "np.save('data/tokenizer.json', tokenizer.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1dfb8-f054-4ae6-8b3f-5a85acd3128a",
   "metadata": {},
   "source": [
    "This notebook demonstrates the complete preprocessing pipeline for spam email detection, including loading data, cleaning text, tokenizing, lemmatizing, and converting text to sequences. The processed data is then saved for future use in training a neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
